# OperationLog Analytics and Reporting

## Overview

The OperationLog analytics system provides comprehensive monitoring, analysis, and reporting capabilities for all operation logs generated by the PriceTracker services (Celery, Fetcher, Extractor).

## Features

### 1. Statistics Service

Provides detailed statistics on operation logs:

- **Overall Metrics**
  - Total log count
  - Success/error/warning counts
  - Success and error rates
  - Time range analysis

- **Distribution Analysis**
  - Logs by level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  - Logs by service (celery, fetcher, extractor)
  - Top events by frequency

- **Performance Metrics**
  - Average, min, max operation duration
  - Performance by event type
  - Slow operation detection (95th percentile)

### 2. Timeline Analysis

Track operations over time:

- **Time-based Aggregations**
  - Hourly, daily, or weekly buckets
  - Success/error trends over time
  - Service activity patterns

- **Task Timeline**
  - Chronological event sequence for specific tasks
  - Elapsed time tracking
  - Duration measurements per event
  - Full context data for debugging

### 3. Failure Reason Reporting

Identify and analyze failures:

- **Error Analysis**
  - Top error events by frequency
  - Common error messages
  - Error distribution by service
  - Error timeline (hourly buckets)

- **Failed Resources**
  - Most failed listings
  - Most failed products
  - Error counts per resource

### 4. Service Health Monitoring

Real-time health dashboard:

- **Health Scoring**
  - Automated health score calculation (100 - error_rate)
  - Status classification (healthy/degraded/unhealthy)
  - Thresholds: healthy ≥90%, degraded ≥70%, unhealthy <70%

- **Service Metrics**
  - Total logs, error counts, success rates
  - Average operation duration
  - Last 24-hour window analysis

## API Reference

### OperationLogService

Basic query operations for OperationLog entries.

#### `get_logs()`

```python
logs = OperationLogService.get_logs(
    service='fetcher',          # Optional: filter by service
    level='ERROR',              # Optional: filter by log level
    event='fetch_page_failed',  # Optional: filter by event
    task_id='abc123',           # Optional: filter by task ID
    listing_id=uuid,            # Optional: filter by listing
    product_id=uuid,            # Optional: filter by product
    time_since=datetime,        # Optional: filter logs after timestamp
    time_until=datetime,        # Optional: filter logs before timestamp
    limit=100                   # Optional: limit results
)
```

#### `get_task_timeline(task_id)`

```python
timeline = OperationLogService.get_task_timeline('abc123')
# Returns: List of events with elapsed time, duration, and context
```

#### `get_events_by_service(service, time_since=None)`

```python
events = OperationLogService.get_events_by_service('fetcher')
# Returns: List of unique event names for the service
```

### OperationLogAnalyticsService

Advanced analytics and reporting.

#### `get_statistics()`

```python
stats = OperationLogAnalyticsService.get_statistics(
    service='fetcher',      # Optional: filter by service
    time_since=datetime,    # Optional: start time
    time_until=datetime     # Optional: end time
)
# Returns: Dictionary with comprehensive statistics
```

**Response Structure:**
```python
{
    'total_logs': int,
    'success_count': int,
    'error_count': int,
    'warning_count': int,
    'success_rate': float,
    'error_rate': float,
    'level_distribution': [{'level': str, 'count': int}, ...],
    'service_distribution': [{'service': str, 'count': int}, ...],
    'top_events': [{'event': str, 'count': int}, ...],
    'duration_stats': {
        'avg_ms': float,
        'max_ms': int,
        'min_ms': int
    },
    'time_range': {
        'first_log': datetime,
        'last_log': datetime
    }
}
```

#### `get_failure_analysis()`

```python
failures = OperationLogAnalyticsService.get_failure_analysis(
    service='fetcher',      # Optional: filter by service
    time_since=datetime,    # Optional: start time
    limit=50                # Number of top failures
)
# Returns: Dictionary with failure analysis
```

**Response Structure:**
```python
{
    'total_failures': int,
    'top_error_events': [{'event': str, 'service': str, 'count': int}, ...],
    'common_error_messages': [{'message': str, 'event': str, 'count': int}, ...],
    'failed_listings': [
        {
            'listing__id': uuid,
            'listing__url': str,
            'listing__product__name': str,
            'listing__store__name': str,
            'error_count': int,
            'last_error': datetime
        },
        ...
    ],
    'failed_products': [...],
    'error_timeline': [{'hour': datetime, 'count': int}, ...]
}
```

#### `get_timeline_analysis()`

```python
timeline = OperationLogAnalyticsService.get_timeline_analysis(
    time_since=datetime,    # Optional: start time
    time_until=datetime,    # Optional: end time
    bucket_size='hour'      # 'hour', 'day', or 'week'
)
# Returns: Time-bucketed analytics
```

#### `get_performance_metrics()`

```python
performance = OperationLogAnalyticsService.get_performance_metrics(
    service='fetcher',      # Optional: filter by service
    event='fetch_page',     # Optional: filter by event
    time_since=datetime     # Optional: start time
)
# Returns: Performance metrics with slow operation detection
```

#### `get_service_health_summary()`

```python
health = OperationLogAnalyticsService.get_service_health_summary()
# Returns: Health metrics for all services (last 24 hours)
```

**Response Structure:**
```python
{
    'timestamp': datetime,
    'period_hours': 24,
    'services': {
        'celery': {
            'total_logs': int,
            'error_count': int,
            'error_rate': float,
            'success_rate': float,
            'health_score': float,
            'avg_duration_ms': float,
            'status': str  # 'healthy', 'degraded', or 'unhealthy'
        },
        'fetcher': {...},
        'extractor': {...}
    }
}
```

## Web UI Endpoints

### Operation Analytics Dashboard

**URL:** `/admin-dashboard/operation-analytics/`

**Query Parameters:**
- `service` - Filter by service (celery, fetcher, extractor)
- `range` - Time range (1h, 24h, 7d, 30d, all)

**Features:**
- Overall statistics (total logs, success/error rates, avg duration)
- Failure analysis with top error events
- Failed listings and products
- Performance metrics by event
- Timeline analysis with time buckets

### Service Health Dashboard

**URL:** `/admin-dashboard/operation-health/`

**Features:**
- Real-time health monitoring (last 24 hours)
- Health scores and status for each service
- Error rates and success rates
- Average operation duration
- Visual status indicators (healthy/degraded/unhealthy)

### Task Timeline Viewer

**URL:** `/admin-dashboard/task/<task_id>/`

**Features:**
- Chronological event timeline for a specific task
- Elapsed time tracking from task start
- Event-level duration measurements
- Expandable context data
- Visual timeline with color-coded events

## Usage Examples

### Example 1: Get Error Statistics for Last 24 Hours

```python
from datetime import timedelta
from django.utils import timezone
from app.operation_log_services import OperationLogAnalyticsService

# Get failure analysis
time_since = timezone.now() - timedelta(hours=24)
failures = OperationLogAnalyticsService.get_failure_analysis(
    service='fetcher',
    time_since=time_since
)

print(f"Total failures: {failures['total_failures']}")
for error in failures['top_error_events'][:5]:
    print(f"- {error['event']}: {error['count']} occurrences")
```

### Example 2: Monitor Service Health

```python
from app.operation_log_services import OperationLogAnalyticsService

# Get health summary
health = OperationLogAnalyticsService.get_service_health_summary()

for service, metrics in health['services'].items():
    status = metrics['status']
    score = metrics['health_score']
    print(f"{service}: {status} (score: {score}%)")
```

### Example 3: Analyze Performance

```python
from app.operation_log_services import OperationLogAnalyticsService

# Get performance metrics
performance = OperationLogAnalyticsService.get_performance_metrics(
    service='fetcher'
)

# Show slowest operations
print("Slowest operations:")
for op in performance['event_performance'][:10]:
    print(f"- {op['event']}: avg {op['avg_duration']:.0f}ms, max {op['max_duration']}ms")
```

### Example 4: Debug a Specific Task

```python
from app.operation_log_services import OperationLogService

# Get task timeline
timeline = OperationLogService.get_task_timeline('abc123-def456')

print(f"Task completed in {timeline[-1]['elapsed_ms']}ms")
for event in timeline:
    print(f"[+{event['elapsed_ms']}ms] {event['event']}: {event['message']}")
```

## Integration with Admin Dashboard

The OperationLog analytics are integrated into the admin dashboard:

1. **Dashboard Overview**
   - Service health summary card showing status of all services
   - Quick links to detailed analytics

2. **System Monitoring Section**
   - Operation Analytics - Comprehensive statistics and failure reporting
   - Service Health - Real-time health dashboard
   - System Logs - Traditional log view (existing)

3. **Quick Access**
   - Click task IDs in logs to view detailed timeline
   - Filter analytics by service and time range
   - Real-time health status updates

## Performance Considerations

- **Indexing:** OperationLog model has indexes on:
  - `service`, `timestamp`
  - `task_id`, `timestamp`
  - `level`, `timestamp`
  - `listing`, `timestamp`
  - `product`, `timestamp`
  - `event`, `timestamp`

- **Query Optimization:**
  - Uses `select_related()` for foreign key joins
  - Limits result sets to prevent large data transfers
  - Time-based filtering for large datasets

- **Caching:** Consider implementing caching for:
  - Service health summary (refresh every 5 minutes)
  - Statistics for fixed time ranges
  - Top error events

## Future Enhancements

Potential additions to the analytics system:

1. **Alerting System**
   - Automatic alerts when health score drops below threshold
   - Email/Slack notifications for critical errors
   - Anomaly detection for unusual patterns

2. **Trends Analysis**
   - Compare metrics across different time periods
   - Identify performance degradation trends
   - Seasonal pattern detection

3. **Export Features**
   - CSV/JSON export of analytics data
   - Scheduled reports via email
   - API endpoints for external monitoring tools

4. **Advanced Visualizations**
   - Charts and graphs for timeline data
   - Heatmaps for error distribution
   - Real-time dashboards with auto-refresh

5. **Custom Metrics**
   - User-defined KPIs
   - Custom aggregations
   - Business-specific analytics
