# PriceFetcher

## Purpose

Deterministic cron job that applies extraction patterns (generated by ExtractorPatternAgent) to fetch product prices on a regular schedule. No AI - purely pattern execution and data storage.

## Architecture

```
Cron Schedule (e.g., every 15 minutes)
    ↓
┌─────────────────────────────────────────┐
│         PriceFetcher Job                │
│                                         │
│  1. Load patterns from storage          │
│  2. Fetch product pages                 │
│  3. Apply extraction patterns           │
│  4. Validate extracted data             │
│  5. Store prices + metadata             │
│  6. Log results                         │
└─────────────────────────────────────────┘
    ↓
Database: Product prices with timestamps
```

## Component Structure

```
PriceFetcher/
├── ARCHITECTURE.md
├── src/
│   ├── fetcher.py           # Main fetcher logic
│   ├── pattern_loader.py    # Load patterns from storage
│   ├── extractor.py         # Apply patterns to HTML
│   ├── validator.py         # Validate extracted data
│   ├── storage.py           # Store price history
│   └── models.py            # Data models
├── config/
│   ├── settings.yaml        # Configuration
│   └── cron.yaml            # Schedule definitions
├── tests/
│   ├── test_fetcher.py
│   └── test_extractor.py
└── scripts/
    ├── run_fetch.py         # Manual execution
    └── setup_cron.sh        # Cron installation
```

## Data Flow

```
1. Pattern Storage (SQLite/JSON)
   ↓
2. Load patterns for active products
   ↓
3. For each product:
   a. Fetch HTML (requests/httpx)
   b. Apply pattern (BeautifulSoup/lxml)
   c. Validate extraction
   d. Store result
   ↓
4. Database (PostgreSQL/SQLite)
   - products table
   - price_history table
   - fetch_logs table
```

## Core Components

### 1. Pattern Loader (pattern_loader.py)

Loads extraction patterns from storage.

```python
from typing import Dict, List, Any
import json
from pathlib import Path

class PatternLoader:
    """Load extraction patterns for products."""

    def __init__(self, storage_path: str = "patterns.db"):
        self.storage_path = storage_path

    def load_pattern(self, domain: str) -> Dict[str, Any]:
        """
        Load pattern for a specific domain.

        Returns:
            {
                "store_domain": "amazon.com",
                "patterns": {
                    "price": {"primary": {...}, "fallbacks": [...]},
                    "title": {...},
                    "availability": {...}
                }
            }
        """
        # Implementation: Query SQLite or read JSON file
        pass

    def load_all_active_patterns(self) -> List[Dict[str, Any]]:
        """Load patterns for all products scheduled for fetching."""
        pass

    def get_products_for_domain(self, domain: str) -> List[Dict[str, Any]]:
        """
        Get all products tracked for a domain.

        Returns list of:
            {
                "product_id": "uuid",
                "url": "https://...",
                "domain": "amazon.com",
                "last_checked": "2025-12-14T10:00:00Z",
                "check_interval": 900  # seconds
            }
        """
        pass
```

### 2. Extractor (extractor.py)

Applies patterns to HTML to extract data.

```python
from bs4 import BeautifulSoup
from lxml import html as lxml_html
from typing import Dict, Any, Optional
import json
import re

class Extractor:
    """Apply extraction patterns to HTML."""

    def extract_with_pattern(
        self,
        html: str,
        pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract data using pattern with fallback support.

        Args:
            html: Page HTML
            pattern: Pattern dict from ExtractorPatternAgent

        Returns:
            {
                "price": {"value": "29.99", "currency": "USD", "method": "css"},
                "title": {"value": "Product Name", "method": "jsonld"},
                "availability": {"value": True, "method": "css"},
                "image": {"value": "https://...", "method": "css"}
            }
        """
        results = {}

        for field_name, field_pattern in pattern["patterns"].items():
            value = self._extract_field(html, field_pattern)
            results[field_name] = value

        return results

    def _extract_field(
        self,
        html: str,
        field_pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Extract single field with fallback chain."""
        # Try primary pattern
        primary = field_pattern["primary"]
        value = self._apply_selector(html, primary)

        if value:
            return {
                "value": value,
                "method": primary["type"],
                "confidence": primary["confidence"]
            }

        # Try fallbacks
        for fallback in field_pattern.get("fallbacks", []):
            value = self._apply_selector(html, fallback)
            if value:
                return {
                    "value": value,
                    "method": fallback["type"],
                    "confidence": fallback["confidence"]
                }

        # All patterns failed
        return {"value": None, "method": None, "confidence": 0.0}

    def _apply_selector(
        self,
        html: str,
        selector_config: Dict[str, Any]
    ) -> Optional[str]:
        """Apply a single selector to HTML."""
        selector_type = selector_config["type"]
        selector = selector_config["selector"]

        try:
            if selector_type == "css":
                return self._extract_css(html, selector, selector_config)
            elif selector_type == "xpath":
                return self._extract_xpath(html, selector, selector_config)
            elif selector_type == "jsonld":
                return self._extract_jsonld(html, selector)
            elif selector_type == "meta":
                return self._extract_meta(html, selector)
        except Exception as e:
            # Log but don't fail - try next fallback
            return None

        return None

    def _extract_css(
        self,
        html: str,
        selector: str,
        config: Dict[str, Any]
    ) -> Optional[str]:
        """Extract using CSS selector."""
        soup = BeautifulSoup(html, 'html.parser')
        element = soup.select_one(selector)

        if not element:
            return None

        # Check if we need attribute value
        if "attribute" in config:
            return element.get(config["attribute"])

        return element.get_text(strip=True)

    def _extract_xpath(
        self,
        html: str,
        selector: str,
        config: Dict[str, Any]
    ) -> Optional[str]:
        """Extract using XPath selector."""
        tree = lxml_html.fromstring(html)
        elements = tree.xpath(selector)

        if not elements:
            return None

        element = elements[0]

        if "attribute" in config:
            return element.get(config["attribute"])

        return element.text_content().strip()

    def _extract_jsonld(self, html: str, path: str) -> Optional[str]:
        """Extract from JSON-LD structured data."""
        soup = BeautifulSoup(html, 'html.parser')
        scripts = soup.find_all('script', type='application/ld+json')

        for script in scripts:
            try:
                data = json.loads(script.string)
                # Navigate path like "offers.price"
                value = self._get_nested(data, path.split('.'))
                if value:
                    return str(value)
            except:
                continue

        return None

    def _extract_meta(self, html: str, tag_name: str) -> Optional[str]:
        """Extract from meta tags."""
        soup = BeautifulSoup(html, 'html.parser')
        meta = soup.find('meta', property=tag_name) or soup.find('meta', attrs={'name': tag_name})

        if meta:
            return meta.get('content')

        return None

    def _get_nested(self, data: dict, path: List[str]) -> Any:
        """Navigate nested dict by path."""
        current = data
        for key in path:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                return None
        return current
```

### 3. Validator (validator.py)

Validates extracted data quality.

```python
from typing import Dict, Any, List
import re
from decimal import Decimal, InvalidOperation

class Validator:
    """Validate extracted product data."""

    def validate_extraction(
        self,
        extraction: Dict[str, Any],
        previous_value: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Validate extracted data.

        Returns:
            {
                "valid": True/False,
                "errors": ["error1", ...],
                "warnings": ["warning1", ...],
                "confidence": 0.85
            }
        """
        errors = []
        warnings = []

        # Validate price
        price_result = self._validate_price(extraction.get("price"))
        if not price_result["valid"]:
            errors.extend(price_result["errors"])
        warnings.extend(price_result.get("warnings", []))

        # Validate title
        title_result = self._validate_title(extraction.get("title"))
        if not title_result["valid"]:
            errors.extend(title_result["errors"])

        # Check for suspicious changes
        if previous_value:
            change_warnings = self._check_suspicious_changes(
                extraction,
                previous_value
            )
            warnings.extend(change_warnings)

        # Calculate overall confidence
        confidence = self._calculate_confidence(extraction, errors, warnings)

        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings,
            "confidence": confidence
        }

    def _validate_price(self, price_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate price field."""
        if not price_data or not price_data.get("value"):
            return {
                "valid": False,
                "errors": ["Price not found"]
            }

        price_str = price_data["value"]
        errors = []
        warnings = []

        # Extract numeric value
        numeric_match = re.search(r'(\d+\.?\d*)', price_str)
        if not numeric_match:
            errors.append("No numeric value in price")
            return {"valid": False, "errors": errors}

        try:
            price_value = Decimal(numeric_match.group(1))

            # Sanity checks
            if price_value <= 0:
                errors.append("Price is zero or negative")
            elif price_value > 100000:
                warnings.append("Price unusually high (>$100k)")
            elif price_value < 0.01:
                warnings.append("Price unusually low (<$0.01)")

        except InvalidOperation:
            errors.append("Invalid price format")

        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings
        }

    def _validate_title(self, title_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate title field."""
        if not title_data or not title_data.get("value"):
            return {
                "valid": False,
                "errors": ["Title not found"]
            }

        title = title_data["value"]
        errors = []

        if len(title) < 3:
            errors.append("Title too short")
        elif len(title) > 500:
            errors.append("Title too long")

        return {"valid": len(errors) == 0, "errors": errors}

    def _check_suspicious_changes(
        self,
        current: Dict[str, Any],
        previous: Dict[str, Any]
    ) -> List[str]:
        """Check for suspicious data changes."""
        warnings = []

        # Price changed significantly
        if "price" in current and "price" in previous:
            curr_price = self._extract_numeric_price(current["price"])
            prev_price = self._extract_numeric_price(previous["price"])

            if curr_price and prev_price:
                change_pct = abs(curr_price - prev_price) / prev_price * 100
                if change_pct > 50:
                    warnings.append(
                        f"Price changed by {change_pct:.1f}% "
                        f"(${prev_price} → ${curr_price})"
                    )

        # Title changed
        if "title" in current and "title" in previous:
            if current["title"]["value"] != previous["title"]["value"]:
                warnings.append("Product title changed")

        return warnings

    def _extract_numeric_price(self, price_data: Dict[str, Any]) -> Optional[Decimal]:
        """Extract numeric price value."""
        if not price_data or not price_data.get("value"):
            return None

        match = re.search(r'(\d+\.?\d*)', price_data["value"])
        if match:
            try:
                return Decimal(match.group(1))
            except:
                pass

        return None

    def _calculate_confidence(
        self,
        extraction: Dict[str, Any],
        errors: List[str],
        warnings: List[str]
    ) -> float:
        """Calculate confidence score."""
        if errors:
            return 0.0

        # Start with pattern confidence
        confidences = []
        for field_data in extraction.values():
            if isinstance(field_data, dict) and "confidence" in field_data:
                confidences.append(field_data["confidence"])

        base_confidence = sum(confidences) / len(confidences) if confidences else 0.5

        # Reduce for warnings
        penalty = len(warnings) * 0.05
        final_confidence = max(0.0, base_confidence - penalty)

        return round(final_confidence, 2)
```

### 4. Storage (storage.py)

Store price history and fetch results.

```python
import sqlite3
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

class PriceStorage:
    """Store product prices and fetch history."""

    def __init__(self, db_path: str = "prices.db"):
        self.db_path = Path(db_path)
        self._init_db()

    def _init_db(self):
        """Initialize database schema."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Products table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS products (
                product_id TEXT PRIMARY KEY,
                url TEXT NOT NULL UNIQUE,
                domain TEXT NOT NULL,
                name TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_checked TIMESTAMP,
                check_interval INTEGER DEFAULT 900,
                pattern_version TEXT,
                active BOOLEAN DEFAULT 1
            )
        """)

        # Price history table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS price_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                product_id TEXT NOT NULL,
                price DECIMAL(10, 2),
                currency TEXT DEFAULT 'USD',
                available BOOLEAN,
                extracted_data JSON,
                confidence REAL,
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (product_id) REFERENCES products(product_id)
            )
        """)

        # Fetch logs table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS fetch_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                product_id TEXT NOT NULL,
                success BOOLEAN NOT NULL,
                extraction_method TEXT,
                errors TEXT,
                warnings TEXT,
                duration_ms INTEGER,
                fetched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (product_id) REFERENCES products(product_id)
            )
        """)

        # Indexes
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_price_history_product
            ON price_history(product_id, recorded_at DESC)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_fetch_logs_product
            ON fetch_logs(product_id, fetched_at DESC)
        """)

        conn.commit()
        conn.close()

    def save_price(
        self,
        product_id: str,
        extraction: Dict[str, Any],
        validation: Dict[str, Any]
    ):
        """Save extracted price to history."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Extract price value
        price_value = None
        currency = "USD"
        if extraction.get("price") and extraction["price"].get("value"):
            import re
            match = re.search(r'(\d+\.?\d*)', extraction["price"]["value"])
            if match:
                price_value = float(match.group(1))

        # Extract availability
        available = True
        if "availability" in extraction:
            avail_text = extraction["availability"].get("value", "").lower()
            available = "stock" in avail_text or "available" in avail_text

        cursor.execute("""
            INSERT INTO price_history
            (product_id, price, currency, available, extracted_data, confidence)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            product_id,
            price_value,
            currency,
            available,
            json.dumps(extraction),
            validation.get("confidence", 0.0)
        ))

        # Update product last_checked
        cursor.execute("""
            UPDATE products
            SET last_checked = CURRENT_TIMESTAMP
            WHERE product_id = ?
        """, (product_id,))

        conn.commit()
        conn.close()

    def log_fetch(
        self,
        product_id: str,
        success: bool,
        extraction_method: str = None,
        errors: List[str] = None,
        warnings: List[str] = None,
        duration_ms: int = None
    ):
        """Log fetch attempt."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO fetch_logs
            (product_id, success, extraction_method, errors, warnings, duration_ms)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            product_id,
            success,
            extraction_method,
            json.dumps(errors or []),
            json.dumps(warnings or []),
            duration_ms
        ))

        conn.commit()
        conn.close()

    def get_products_to_fetch(self) -> List[Dict[str, Any]]:
        """Get products due for fetching."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT product_id, url, domain, check_interval
            FROM products
            WHERE active = 1
            AND (
                last_checked IS NULL
                OR datetime(last_checked, '+' || check_interval || ' seconds') <= datetime('now')
            )
        """)

        products = []
        for row in cursor.fetchall():
            products.append({
                "product_id": row[0],
                "url": row[1],
                "domain": row[2],
                "check_interval": row[3]
            })

        conn.close()
        return products

    def get_latest_price(self, product_id: str) -> Optional[Dict[str, Any]]:
        """Get most recent price for product."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT price, currency, available, extracted_data, confidence, recorded_at
            FROM price_history
            WHERE product_id = ?
            ORDER BY recorded_at DESC
            LIMIT 1
        """, (product_id,))

        row = cursor.fetchone()
        conn.close()

        if row:
            return {
                "price": row[0],
                "currency": row[1],
                "available": row[2],
                "extracted_data": json.loads(row[3]),
                "confidence": row[4],
                "recorded_at": row[5]
            }

        return None
```

### 5. Main Fetcher (fetcher.py)

Orchestrates the fetch process.

```python
import asyncio
import httpx
from typing import Dict, Any, List
from datetime import datetime
import time

class PriceFetcher:
    """Main price fetcher orchestrator."""

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.pattern_loader = PatternLoader()
        self.extractor = Extractor()
        self.validator = Validator()
        self.storage = PriceStorage()

    async def fetch_all(self) -> Dict[str, Any]:
        """Fetch prices for all products due for update."""
        products = self.storage.get_products_to_fetch()

        results = {
            "total": len(products),
            "success": 0,
            "failed": 0,
            "products": []
        }

        # Group by domain to apply rate limiting
        by_domain = {}
        for product in products:
            domain = product["domain"]
            if domain not in by_domain:
                by_domain[domain] = []
            by_domain[domain].append(product)

        # Fetch per domain with rate limiting
        for domain, domain_products in by_domain.items():
            pattern = self.pattern_loader.load_pattern(domain)

            if not pattern:
                print(f"No pattern found for {domain}, skipping...")
                results["failed"] += len(domain_products)
                continue

            for product in domain_products:
                result = await self.fetch_product(product, pattern)
                results["products"].append(result)

                if result["success"]:
                    results["success"] += 1
                else:
                    results["failed"] += 1

                # Rate limiting: wait between requests
                await asyncio.sleep(self.config.get("request_delay", 2))

        return results

    async def fetch_product(
        self,
        product: Dict[str, Any],
        pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Fetch price for single product."""
        start_time = time.time()
        product_id = product["product_id"]
        url = product["url"]

        try:
            # Fetch HTML
            html = await self._fetch_html(url)

            # Extract data
            extraction = self.extractor.extract_with_pattern(html, pattern)

            # Get previous value for comparison
            previous = self.storage.get_latest_price(product_id)
            previous_data = previous["extracted_data"] if previous else None

            # Validate
            validation = self.validator.validate_extraction(
                extraction,
                previous_data
            )

            # Store if valid
            if validation["valid"]:
                self.storage.save_price(product_id, extraction, validation)

            # Log fetch attempt
            duration_ms = int((time.time() - start_time) * 1000)
            self.storage.log_fetch(
                product_id,
                success=validation["valid"],
                extraction_method=extraction.get("price", {}).get("method"),
                errors=validation.get("errors"),
                warnings=validation.get("warnings"),
                duration_ms=duration_ms
            )

            return {
                "product_id": product_id,
                "url": url,
                "success": validation["valid"],
                "extraction": extraction,
                "validation": validation,
                "duration_ms": duration_ms
            }

        except Exception as e:
            # Log failure
            duration_ms = int((time.time() - start_time) * 1000)
            self.storage.log_fetch(
                product_id,
                success=False,
                errors=[str(e)],
                duration_ms=duration_ms
            )

            return {
                "product_id": product_id,
                "url": url,
                "success": False,
                "error": str(e),
                "duration_ms": duration_ms
            }

    async def _fetch_html(self, url: str) -> str:
        """Fetch HTML from URL."""
        timeout = self.config.get("timeout", 30)
        user_agent = self.config.get(
            "user_agent",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )

        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.get(
                url,
                headers={"User-Agent": user_agent},
                follow_redirects=True
            )
            response.raise_for_status()
            return response.text
```

## Configuration (config/settings.yaml)

```yaml
fetcher:
  request_delay: 2  # seconds between requests
  timeout: 30       # request timeout
  max_retries: 3
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

storage:
  database: "prices.db"
  patterns_database: "patterns.db"

logging:
  level: "INFO"
  file: "logs/fetcher.log"
  rotation: "1 day"
  retention: "30 days"

rate_limits:
  default: 2  # requests per second
  amazon.com: 1
  ebay.com: 1.5
  walmart.com: 1

validation:
  min_confidence: 0.6
  max_price_change_pct: 50  # alert if price changes >50%
```

## Cron Configuration (config/cron.yaml)

```yaml
schedules:
  # High priority products - every 15 minutes
  high_priority:
    interval: "*/15 * * * *"
    query: "SELECT * FROM products WHERE priority = 'high' AND active = 1"

  # Normal priority - every hour
  normal_priority:
    interval: "0 * * * *"
    query: "SELECT * FROM products WHERE priority = 'normal' AND active = 1"

  # Low priority - every 6 hours
  low_priority:
    interval: "0 */6 * * *"
    query: "SELECT * FROM products WHERE priority = 'low' AND active = 1"

  # Cleanup old logs - daily at 2am
  cleanup:
    interval: "0 2 * * *"
    task: "cleanup_old_logs"
    retention_days: 30
```

## Execution Scripts

### scripts/run_fetch.py

```python
#!/usr/bin/env python3
"""
Manual execution script for price fetching.
Usage: python scripts/run_fetch.py [--product-id ID] [--domain DOMAIN]
"""

import asyncio
import argparse
from src.fetcher import PriceFetcher
import json

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--product-id", help="Fetch specific product")
    parser.add_argument("--domain", help="Fetch all products from domain")
    parser.add_argument("--all", action="store_true", help="Fetch all due products")
    args = parser.parse_args()

    fetcher = PriceFetcher()

    if args.all:
        results = await fetcher.fetch_all()
        print(json.dumps(results, indent=2))
    elif args.product_id:
        # Fetch specific product
        pass
    elif args.domain:
        # Fetch domain
        pass

if __name__ == "__main__":
    asyncio.run(main())
```

### scripts/setup_cron.sh

```bash
#!/bin/bash
# Install cron jobs for price fetching

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"

# High priority - every 15 minutes
echo "*/15 * * * * cd $PROJECT_DIR && python scripts/run_fetch.py --all >> logs/cron.log 2>&1" | crontab -

# Or use systemd timers for better control
```

## Integration with ExtractorPatternAgent

```
ExtractorPatternAgent
    ↓
Generates patterns → patterns.db
    ↓
PriceFetcher (cron)
    ↓
Reads patterns → Fetches prices → prices.db
```

## Monitoring & Observability

### Metrics to Track

1. **Fetch Success Rate**: % of successful extractions
2. **Pattern Effectiveness**: Which patterns work best
3. **Response Times**: Latency per domain
4. **Data Quality**: Confidence scores over time
5. **Price Change Frequency**: How often prices change

### Alerting

- Fetch success rate drops below 80%
- Pattern extraction fails 3 times in a row
- Price change exceeds threshold
- Website structure change detected (all patterns fail)

## Testing

```python
# tests/test_fetcher.py
import pytest
from src.fetcher import PriceFetcher
from src.extractor import Extractor

def test_extract_price():
    html = load_fixture("amazon_product.html")
    pattern = load_fixture("amazon_pattern.json")

    extractor = Extractor()
    result = extractor.extract_with_pattern(html, pattern)

    assert result["price"]["value"] is not None
    assert "29.99" in result["price"]["value"]

@pytest.mark.asyncio
async def test_fetch_product():
    fetcher = PriceFetcher()
    # Mock HTTP request
    result = await fetcher.fetch_product(...)
    assert result["success"] is True
```

## Dependencies

```txt
# requirements.txt
httpx>=0.24.0          # Async HTTP client
beautifulsoup4>=4.12.0 # HTML parsing
lxml>=4.9.0            # XPath support
pyyaml>=6.0            # Config files
pydantic>=2.0.0        # Data validation
schedule>=1.2.0        # Cron-like scheduling (optional)
```

## Deployment

### Docker

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

# Run as cron job
CMD ["python", "-m", "schedule", "config/cron.yaml"]
```

### Systemd Timer (alternative to cron)

```ini
# /etc/systemd/system/price-fetcher.timer
[Unit]
Description=Price Fetcher Timer

[Timer]
OnCalendar=*/15 * * * *
Persistent=true

[Install]
WantedBy=timers.target
```

## Next Steps

1. Implement core components (extractor, validator, storage)
2. Add comprehensive tests with fixtures
3. Build monitoring dashboard
4. Add alerting for pattern failures
5. Implement retry logic with exponential backoff
