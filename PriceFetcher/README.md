# PriceFetcher

Deterministic background process for fetching product prices using extraction patterns. No AI - purely pattern execution and data storage.

## Features

- **Python extractor modules**: Uses Python extractors generated by ExtractorPatternAgent
- **Deterministic extraction**: Generated Python code for reliable data extraction
- **Validation**: Ensures extracted data quality before storing
- **Rate limiting**: Respects domain-specific request limits
- **Retry logic**: Handles network errors with exponential backoff
- **Structured logging**: JSON logs for easy monitoring
- **Shared database**: Works with Django SQLite database

## Architecture

```
┌─────────────────────────────────────┐
│  Cron Job (every 15 min)            │
└──────────────┬──────────────────────┘
               ↓
┌──────────────────────────────────────┐
│  PriceFetcher                        │
│  1. Get products due for checking    │
│  2. Fetch HTML with stealth browser  │
│  3. Execute Python extractor         │
│  4. Validate extractions             │
│  5. Store prices & track versions    │
└──────────────┬───────────────────────┘
               ↓
┌──────────────────────────────────────┐
│  Shared SQLite Database              │
│  - app_product                       │
│  - app_pricehistory                  │
│  - app_operationlog                  │
│  - app_extractorversion              │
└──────────────────────────────────────┘
```

## Installation

### Using uv (recommended)

```bash
# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# No additional setup needed - dependencies will be installed on first run
```


## Usage

### Python API (Recommended for Celery Integration)

The `celery_api.py` module provides async functions for direct integration with Django Celery tasks:

```python
from PriceFetcher.src.celery_api import fetch_listing_price_direct, backfill_images_direct

# Fetch a single listing
result = await fetch_listing_price_direct(
    listing_id="123e4567-e89b-12d3-a456-426614174000",
    db_path="/path/to/db.sqlite3"
)

# Backfill missing images
stats = await backfill_images_direct(
    db_path="/path/to/db.sqlite3",
    limit=50,
    request_delay=2.0
)
```

**Celery Integration Example:**

```python
from celery import shared_task
from PriceFetcher.src.celery_api import fetch_listing_price_direct
from django.conf import settings
import asyncio

@shared_task
def fetch_listing_price(listing_id: str):
    return asyncio.run(_fetch_async(listing_id))

async def _fetch_async(listing_id: str):
    db_path = str(settings.DATABASES['default']['NAME'])
    result = await fetch_listing_price_direct(
        listing_id=listing_id,
        db_path=db_path
    )
    return result
```

### CLI Scripts (For Manual Testing)

#### Manual Execution

```bash
# Fetch all products due for checking
uv run scripts/run_fetch.py

# Enable debug logging
uv run scripts/run_fetch.py --verbose

# Output as JSON
uv run scripts/run_fetch.py --json

# Use custom database path
uv run scripts/run_fetch.py --db-path /path/to/db.sqlite3

# Use custom config
uv run scripts/run_fetch.py --config /path/to/config.yaml
```

**Note:** For production use with Celery, prefer the Python API over CLI scripts.

### Automated Execution (Cron)

```bash
# Setup cron jobs
./scripts/setup_cron.sh

# View cron jobs
crontab -l

# Edit cron jobs
crontab -e
```

The default cron schedule:
- Every 15 minutes: Fetch all products due for checking
- Daily at 2am: Cleanup old log files

### Docker

```bash
# Build image
docker build -t pricefetcher .

# Run manually
docker run --rm -v $(pwd)/../db.sqlite3:/app/db.sqlite3 pricefetcher

# Run as cron job (use docker-compose)
docker-compose up -d
```

## API Reference

### Celery API Functions

The `celery_api.py` module provides async functions designed for Celery task integration.

#### `async fetch_listing_price_direct(listing_id: str, db_path: str, config_path: Optional[str] = None) -> Dict`

Fetch price for a specific product listing.

**Parameters:**
- `listing_id` (str): ProductListing UUID (with or without hyphens)
- `db_path` (str): Path to shared SQLite database
- `config_path` (Optional[str]): Path to config file (uses default if not provided)

**Returns:**
- `Dict`: Result dictionary containing:
  - `status` (str): 'success', 'failed', or 'error'
  - `listing_id` (str): Listing UUID
  - `product_id` (str): Product UUID (if found)
  - `url` (str): Product URL (if found)
  - `extraction` (dict): Extracted data (if successful)
  - `validation` (dict): Validation results (if successful)
  - `duration_ms` (int): Fetch duration in milliseconds
  - `error` (str): Error message (if failed)

**Example:**
```python
from PriceFetcher.src.celery_api import fetch_listing_price_direct

result = await fetch_listing_price_direct(
    listing_id="123e4567-e89b-12d3-a456-426614174000",
    db_path="/app/db.sqlite3"
)

if result['status'] == 'success':
    print(f"Price: {result['extraction']['price']}")
    print(f"Confidence: {result['validation']['confidence']}")
```

#### `async fetch_all_due_prices(db_path: str, config_path: Optional[str] = None) -> Dict`

Fetch prices for all products due for checking.

**Parameters:**
- `db_path` (str): Path to shared SQLite database
- `config_path` (Optional[str]): Path to config file (uses default if not provided)

**Returns:**
- `Dict`: Summary dictionary containing:
  - `status` (str): 'success' or 'error'
  - `total` (int): Total products processed
  - `success` (int): Successful fetches
  - `failed` (int): Failed fetches
  - `duration_seconds` (float): Total duration
  - `products` (list): List of product results
  - `error` (str): Error message (if failed)

**Example:**
```python
from PriceFetcher.src.celery_api import fetch_all_due_prices

summary = await fetch_all_due_prices(db_path="/app/db.sqlite3")
print(f"Processed {summary['total']} products")
print(f"Success rate: {summary['success']}/{summary['total']}")
```

#### `async backfill_images_direct(db_path: str, limit: int = 50, request_delay: float = 2.0) -> Dict`

Backfill images for products that don't have them.

**Parameters:**
- `db_path` (str): Path to shared SQLite database
- `limit` (int): Maximum number of products to process (default: 50)
- `request_delay` (float): Delay between requests in seconds (default: 2.0)

**Returns:**
- `Dict`: Statistics dictionary containing:
  - `status` (str): 'success' or 'error'
  - `total` (int): Total products processed
  - `success` (int): Successfully fetched images
  - `failed` (int): Failed fetches
  - `skipped` (int): Products already with images
  - `no_pattern` (int): Products without patterns
  - `duration_seconds` (float): Total duration
  - `error` (str): Error message (if failed)

**Example:**
```python
from PriceFetcher.src.celery_api import backfill_images_direct

stats = await backfill_images_direct(
    db_path="/app/db.sqlite3",
    limit=100,
    request_delay=1.5
)
print(f"Backfilled {stats['success']}/{stats['total']} images")
```

## Configuration

Edit `config/settings.yaml`:

```yaml
fetcher:
  request_delay: 2.0      # Delay between requests (seconds)
  timeout: 30.0           # HTTP timeout (seconds)
  max_retries: 3          # Retry attempts

validation:
  min_confidence: 0.6     # Minimum confidence threshold
  max_price_change_pct: 50.0  # Warning threshold for price changes

rate_limits:
  domains:
    amazon.com: 1.0       # Requests per second
    ebay.com: 0.67
```

Environment variables (override config):
- `DATABASE_PATH`: Path to SQLite database
- `LOG_LEVEL`: Logging level (DEBUG, INFO, WARNING, ERROR)
- `MIN_CONFIDENCE`: Minimum confidence threshold

## Components

### Core Modules

- **`fetcher.py`**: Main orchestrator
  - Loads products due for checking
  - Fetches HTML
  - Coordinates extraction and validation
  - Stores results

- **`extractor.py`**: Python extractor execution
  - Loads and executes generated Python extractors
  - Extracts price, title, availability, image
  - Tracks extractor module versions

- **`validator.py`**: Data validation
  - Validates price format and sanity
  - Checks for suspicious changes
  - Calculates confidence scores

- **`storage.py`**: Database layer
  - Stores price history
  - Logs operations to app_operationlog
  - Tracks extractor versions

### Data Flow

```
1. Celery Beat triggers fetch_prices_by_aggregated_priority every 5 minutes
2. PriorityAggregationService determines which products are due for checking
3. For each product listing:
   a. Load Python extractor module for the store domain
   b. Fetch HTML with stealth browser (rate limiting applied)
   c. Execute Python extractor to extract data
   d. Validate extraction
   e. Store in app_pricehistory
   f. Log operation in app_operationlog
   g. Track extractor version in app_extractorversion
   h. Update app_productlisting.last_checked
```

## Database Schema

Uses Django-created tables (see WebUI/app/models.py):

- **`app_product`**: Normalized product entities
- **`app_store`**: Store/retailer information
- **`app_productlisting`**: Product listings per store (URLs, prices)
- **`app_pricehistory`**: Historical prices per listing
- **`app_operationlog`**: Operation logs (fetch attempts, errors, etc.)
- **`app_extractorversion`**: Git version tracking for Python extractors

## Logging

Structured JSON logs with:
- Timestamp (ISO format)
- Log level
- Event name
- Contextual data

Example log entry:
```json
{
  "timestamp": "2025-12-14T10:30:00Z",
  "level": "info",
  "event": "product_fetch_completed",
  "product_id": "123e4567-e89b-12d3-a456-426614174000",
  "success": true,
  "price": "29.99",
  "confidence": 0.95,
  "duration_ms": 1234
}
```

## Monitoring

Key metrics to track:
- Fetch success rate (%)
- Average confidence score
- Latency per domain (ms)
- Pattern effectiveness
- Failed extractions by domain

Export logs to monitoring system (e.g., Prometheus, Grafana, DataDog).

## Error Handling

### Common Errors

1. **No extractor found for domain**
   - Error: "No extractor found for domain"
   - Trigger ExtractorPatternAgent to generate Python extractor
   - Products marked as failed until extractor created

2. **HTTP errors**
   - 4xx: Don't retry (bad URL, not found)
   - 5xx: Retry with exponential backoff
   - Timeouts: Retry up to max_retries

3. **Extraction failures**
   - Log failure to app_operationlog
   - Extractor may need regeneration if site structure changed

4. **Validation failures**
   - Price not found / invalid format
   - Confidence below threshold
   - Logged but not stored

## Development

### Project Structure

```
PriceFetcher/
├── src/
│   ├── __init__.py
│   ├── fetcher.py        # Main orchestrator
│   ├── extractor.py      # Python extractor execution
│   ├── validator.py      # Data validation
│   ├── storage.py        # Database layer
│   └── models.py         # Pydantic models
├── config/
│   ├── __init__.py
│   └── settings.yaml     # Configuration
├── scripts/
│   ├── run_fetch.py      # Manual execution
│   └── setup_cron.sh     # Cron setup
├── tests/
│   └── (test files)
├── logs/
│   └── (log files)
├── pyproject.toml        # Package config
└── README.md
```

### Running Tests

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest

# Run with coverage
pytest --cov=src
```

### Code Quality

```bash
# Format code
ruff format .

# Lint code
ruff check .

# Type checking
mypy src/
```

## Deployment

Run with Docker Compose from the project root:

```bash
cd ..
docker-compose up -d
```

The PriceFetcher runs as part of the Celery worker service. See `docker-compose.yml` for configuration.

## Troubleshooting

### Database not found

```
Error: Database not found at ../db.sqlite3
```

**Solution**: Run Django migrations first:
```bash
cd ../WebUI
python manage.py migrate
```

### No extractor found for domain

```
Warning: No extractor found for domain amazon.com
```

**Solution**: Trigger ExtractorPatternAgent to generate Python extractor:
```bash
cd ../ExtractorPatternAgent
# Use the ExtractorPatternAgent CLI or API to generate extractor
# See ExtractorPatternAgent/README.md for current generation commands
```

### Low confidence scores

```
Warning: Confidence 0.45 below threshold 0.6
```

**Solution**:
1. Check if website structure changed
2. Regenerate pattern with ExtractorPatternAgent
3. Adjust `min_confidence` threshold in config

### Rate limiting / IP blocks

```
Error: HTTP 429 Too Many Requests
```

**Solution**:
1. Increase `request_delay` in config
2. Use proxy rotation (future feature)
3. Reduce fetch frequency per domain

## License

See repository LICENSE file.

## Contributing

1. Follow existing code style
2. Add tests for new features
3. Update documentation
4. Use structured logging
5. Handle errors gracefully
